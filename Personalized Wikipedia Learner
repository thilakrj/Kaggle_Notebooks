{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaA100","dataSources":[{"sourceId":11363773,"sourceType":"datasetVersion","datasetId":7112711},{"sourceId":11534352,"sourceType":"datasetVersion","datasetId":7234250},{"sourceId":363131,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":301511,"modelId":322000}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sripalthilakraj/personalized-wikipedia-learner?scriptVersionId=239506111\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **üìö Building a Personalized Wikipedia Learning Experience with LLMs**\n\nThe Notebook aims to transform the vast amount of information available in the Wikipedia dataset into a structured, personalized learning journey. The core challenge addressed is the difficulty users often face in navigating Wikipedia, understanding the relationships between articles, and establishing a logical starting point for learning a complex topic.\n\nThis notebook will guide you through the process of:\n\n1. **Identifying Relevant Articles**: The initial step involves pinpointing the most pertinent Wikipedia articles related to a chosen topic.\n2. **Structuring the Learning Path**: Utilizing an LLM, the identified articles are arranged into a logical, step-by-step learning sequence. This includes generating summaries for each article and suggesting prerequisites to ensure a smooth learning flow.\n\n### Let's get started! üëá\n\nChallenge here is Navigating Massive Datasets\n\n**Navigator Dataset** acts as an efficient index for the massive Wikipedia data. Instead of requiring a search through the entire raw dataset, the Navigator Dataset provides precise locations (file and line number) for the full content of millions of articles. This significantly improves the speed and efficiency of accessing specific article content in real-time, which is presented as a solution to the challenge of navigating such a large dataset.","metadata":{}},{"cell_type":"markdown","source":"## 1. üõ†Ô∏è Importing Necessary Libraries and Datasets","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport os\nimport torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Path to the navigator dataset file\nnavigator_file_path = \"/kaggle/input/wikimedia-structured-dataset-navigator-jsonl/wiki_structured_dataset_navigator.jsonl\"\n\n# Path to the directory containing the full Wikipedia structured content files. These files are named like 'enwiki_namespace_0_0.jsonl', 'enwiki_namespace_0_1.jsonl', etc.\nfull_dataset_dir = \"/kaggle/input/wikipedia-structured-contents/enwiki_namespace_0/\"\n\nprint(f\"Navigator file path: {navigator_file_path}\")\nprint(f\"Full dataset directory: {full_dataset_dir}\")\nprint(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:58:50.984869Z","iopub.execute_input":"2025-05-13T10:58:50.985493Z","iopub.status.idle":"2025-05-13T10:59:15.024212Z","shell.execute_reply.started":"2025-05-13T10:58:50.985468Z","shell.execute_reply":"2025-05-13T10:59:15.023659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Explore the Navigator Dataset üß≠","metadata":{}},{"cell_type":"code","source":"navigator_data = []\nwith open(navigator_file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            navigator_data.append(json.loads(line))\n\n\nif navigator_data:\n    navigator_df = pd.DataFrame(navigator_data)\n\n    print(\"Navigator dataset loaded successfully.\")\n    print(f\"Total articles indexed in navigator: {len(navigator_df)}\")\n    print(\"\\nFirst 5 entries from the navigator:\")\n    print(navigator_df.head())\n    print(\"\\nColumns in the navigator dataset:\")\n    print(navigator_df.columns)\n    print(\"-\" * 30)\nelse:\n    print(\"Navigator data is empty. Cannot proceed with article selection.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:59:15.025135Z","iopub.execute_input":"2025-05-13T10:59:15.025597Z","iopub.status.idle":"2025-05-13T10:59:53.11165Z","shell.execute_reply.started":"2025-05-13T10:59:15.025576Z","shell.execute_reply":"2025-05-13T10:59:53.111032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. üöÄ Configuring and Loading the Model","metadata":{}},{"cell_type":"code","source":"# Path to the Qwen 3 model on Kaggle\nQWEN_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/1.7b/1\" \n\nMODEL_CONFIG = {\n    \"torch_dtype\": torch.bfloat16, \n    \"device_map\": \"auto\",\n    \"trust_remote_code\": True \n}\n\n# --- Load Qwen 3 Model ---\nprint(\"Loading Qwen 3 model...\")\ntokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_PATH, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(QWEN_MODEL_PATH, **MODEL_CONFIG)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\nprint(\"Qwen 3 model loaded successfully.\")\nprint(\"-\" * 30)\n\n# --- Setup the requirement depends on task-- \nNUM_ARTICLES_FOR_PATH = 5 # Aim for  => 5 relevant articles\nNUM_QUIZ_QUESTIONS_PER_ARTICLE = 2 \nMAX_LLM_TOKENS_PATH = 1300 \nMAX_LLM_TOKENS_QUIZ = 400 \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:59:53.112242Z","iopub.execute_input":"2025-05-13T10:59:53.112454Z","iopub.status.idle":"2025-05-13T11:00:23.525631Z","shell.execute_reply.started":"2025-05-13T10:59:53.112436Z","shell.execute_reply":"2025-05-13T11:00:23.524927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. üîç  Getting user_topic and Querying in navigator dataset","metadata":{}},{"cell_type":"code","source":"selected_articles_info = []\nuser_topic = \"Psychology\"\n#user_topic = input(\"Enter the topic you want to learn about: \") \nprint(f\"\\nSearching for articles related to: '{user_topic}'\")\nname_search = navigator_df['name'].str.contains(user_topic, case=False, na=False) if 'name' in navigator_df.columns else pd.Series(False, index=navigator_df.index)\ndesc_search = navigator_df['description'].str.contains(user_topic, case=False, na=False) if 'description' in navigator_df.columns and 'description' in navigator_df.iloc[0].keys() else pd.Series(False, index=navigator_df.index) # Check if description column exists and has data\nrelevant_articles_df = navigator_df[name_search | desc_search]\n\n#Selecting article -   Take the top N based on index sample  or Take all.\nif len(relevant_articles_df) > NUM_ARTICLES_FOR_PATH:\n    selected_articles_info = relevant_articles_df.head(NUM_ARTICLES_FOR_PATH).to_dict('records')\n    print(f\"Found {len(relevant_articles_df)} articles matching '{user_topic}'. Selecting the top {NUM_ARTICLES_FOR_PATH}...\")\nelse:\n    selected_articles_info = relevant_articles_df.to_dict('records')\n    print(f\"Found {len(relevant_articles_df)} articles matching '{user_topic}'. Selecting all available...\")\n\nprint(\"Selected articles for retrieval:\")\nfor i, article_info in enumerate(selected_articles_info):\n    print(f\"  {i+1}. Name: {article_info.get('name', 'N/A')}, File: {article_info.get('file_name', 'N/A')}, Index: {article_info.get('file_index', 'N/A')}\")\n    print(\"-\" * 30)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:00:23.526895Z","iopub.execute_input":"2025-05-13T11:00:23.52733Z","iopub.status.idle":"2025-05-13T11:00:33.343992Z","shell.execute_reply.started":"2025-05-13T11:00:23.52731Z","shell.execute_reply":"2025-05-13T11:00:33.343426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. üì• Retriving articles from the Wikipedia-structure-contents dataset. ","metadata":{}},{"cell_type":"code","source":"retrieved_full_articles = []\n\nprint(\"\\nAttempting to retrieve full content for selected articles...\\n\")\nfor i, article_info in enumerate(selected_articles_info):\n    article_name = article_info.get('name', 'Unknown Article')\n    main_data_file = article_info.get('file_name')\n    article_line_index = article_info.get('file_index')\n    \n    # Checking article name in fulldataset\n    full_article_file_path = os.path.join(full_dataset_dir, main_data_file)\n    print(f\"Processing article: '{article_name}'\")\n    \n    with open(full_article_file_path, 'r', encoding='utf-8') as f_main:\n        found_line = False\n        # Read lines until we reach the desired index\n        for current_index, line in enumerate(f_main):\n            if current_index == article_line_index:\n                full_article_content = json.loads(line)\n                retrieved_full_articles.append(full_article_content)\n                print(f\"  Successfully retrieved content for '{article_name}'.\")\n                found_line = True\n                break\n        print(\"-\" * 20)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:00:33.344589Z","iopub.execute_input":"2025-05-13T11:00:33.344778Z","iopub.status.idle":"2025-05-13T11:00:36.329992Z","shell.execute_reply.started":"2025-05-13T11:00:33.344763Z","shell.execute_reply":"2025-05-13T11:00:36.329422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. üó∫Ô∏è Crafting AI Learning Path","metadata":{}},{"cell_type":"code","source":"MAX_LLM_TOKENS_PATH =2000\nprint(\"\\n--- Generating AI Learning Path ---\")\n# Prepare data for LLM - pass only necessary parts to save context window\narticles_summary_for_llm = []\nfor article in retrieved_full_articles:\n    # Limit abstract if they are very long\n    abstract_snippet = article.get('abstract', 'No abstract available')\n    abstract_snippet = abstract_snippet[:1000] + \"...\" if len(abstract_snippet) > 1000 else abstract_snippet\n    sections_info = []\n    if 'sections' in article and article['sections']:\n        sections_info = [s.get('name', 'Unnamed Section') for s in article['sections'] if s.get('name')]\n        # Preparing data for article summarization\n        articles_summary_for_llm.append({\n            \"name\": article.get('name', 'N/A'),\n            \"abstract\": abstract_snippet,\n            \"sections_overview\": sections_info\n        })\n\n    # Format the articles data into a string for the prompt\n    articles_text_for_path = \"\\n\\n\".join([\n        f\"## Title: {a['name']}\\nAbstract: {a['abstract']}\\nSections: {', '.join(a['sections_overview'])}\" for a in articles_summary_for_llm\n    ])\n\n# Prompt construction for learning path generation\nllm_prompt_path = f\"\"\"You are an AI educator. Your task is to create a learning path about \"{user_topic}\" using ONLY the specific Wikipedia articles provided below.\n\n**Context & Constraints:**\n* You MUST only use the articles listed. Do not suggest or refer to any external articles.\n* The provided list may not cover all foundational aspects of \"{user_topic}\" for a complete beginner. You can briefly mention this.\n\n**Provided Articles Information (Title, URL, Abstract, Main Sections):**\n{articles_text_for_path}\n\n**Your Output Requirements:**\n\n1.  **Prerequisite Knowledge (Optional but Recommended):**\n    * Briefly suggest 1-2 general prerequisite topics or knowledge areas the user should ideally be familiar with before starting this specific learning path, especially if the provided articles seem advanced. Keep this section very concise.\n\n2.  **Learning Path:**\n    * Suggest a logical reading order for the provided articles to learn about \"{user_topic}\".\n    * Present this path as a numbered list.\n    * For EACH article in your suggested order, you MUST provide the following details in this exact format:\n        a.  **Title:** The exact article title (from the provided list).\n        b.  **URL:** The direct URL to the Wikipedia article (use the URL provided in the article information).\n        c.  **Summary:** A brief 1-2 sentence summary explaining its relevance to *this specific* learning path and \"{user_topic}\". Focus on why it's useful at this point in the learning sequence.\n        d.  **Placement Reason:** A very short phrase (e.g., \"Fundamental overview,\" \"Core theory,\" \"Practical application,\" \"Specific influence,\" \"Key resource,\" \"Real-world case\") explaining why it's placed at that step in the order.\n\n\"\"\" \nmessages_path = [{\"role\": \"user\", \"content\": llm_prompt_path}]\n\n# Apply the chat template with enable_thinking=False\n# This will return a string because tokenize=False\nformatted_prompt_for_path = tokenizer.apply_chat_template(\n    messages_path,\n    tokenize=False,\n    add_generation_prompt=True, \n    enable_thinking=False      # Disable thinking mode\n)\n\nprint(\"Sending learning path request to LLM...\")\nllm_response_path = pipe(\n    formatted_prompt_for_path,\n    max_new_tokens=MAX_LLM_TOKENS_PATH,\n    num_return_sequences=1,\n    do_sample=True, # Use sampling for potentially more creative responses\n    temperature=0.7,\n    top_p=0.9, \n    pad_token_id=tokenizer.eos_token_id \n    )[0]['generated_text']\n\n# The response will include the prompt itself, extract only the new part\ngenerated_path_text = llm_response_path[len(formatted_prompt_for_path):].strip()\nprint(\"\\n--- AI Learning Path ---\")\nprint(generated_path_text)\nprint(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:02:50.699549Z","iopub.execute_input":"2025-05-13T11:02:50.700241Z","iopub.status.idle":"2025-05-13T11:03:09.109703Z","shell.execute_reply.started":"2025-05-13T11:02:50.700215Z","shell.execute_reply":"2025-05-13T11:03:09.108947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üéØ Conclusion**\nWe've successfully built a system that takes a user-defined topic, intelligently retrieves relevant Wikipedia articles using a navigator, generates a structured learning path with summaries by a large language model!\n\n### üí° What's Next? (Ideas for Improvement)\n\n- Better Article Ranking: Implement more sophisticated methods to rank article relevance beyond simple keyword matching (e.g., using embeddings, link analysis).\n\n- Different LLM Tasks: Explore other ways LLMs can enhance learning, such as generating flashcards, explaining complex terms, or creating concept maps based on the articles.\n\n- User Interface: Build a simple web interface to make the topic input and output (learning path, quiz) more user-friendly.\n","metadata":{}},{"cell_type":"markdown","source":"## **üôè Acknowledgements**\nA huge thank you and credit to the author of the kernel \"How to easily search the Wikimedia data\" (https://www.kaggle.com/code/mehranism/how-to-easily-search-the-wikimedia-data). The idea and implementation of the navigator dataset are truly wonderful and were essential in making this project feasible by allowing real-time access to specific article content within this massive dataset. Thank you for sharing this valuable technique!\n\nHappy Learning! üéâ","metadata":{}}]}